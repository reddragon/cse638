\documentclass{article}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{url}
\usepackage{color}
\usepackage{savetrees}
\usepackage{listings}
\usetikzlibrary{shapes}


\linespread{1.5}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1.9ex plus 0.5ex minus 0.2ex}

\title{CSE638 \\ Advanced Graduate Algorithms (and Data Structures) -- Homework-1}

\author{Dhruv Matani (108267786) \& Gaurav Menghani (108266803)}

\begin{document}
\maketitle

\clearpage

\tableofcontents

\clearpage

% $\frac{\frac{\frac{a}{b}}{c}}{\sqrt{\sqrt{{x}^{y}}}}$

\section{Majority Problem}
Let us select a sample of $s = c\log{n}$ elements. In the worst-case, we can
have the array with just two types of elements, the majority element and the
non-majority element.

Now, after drawing the sample of $s$ elements, let the random variables
$x_{1}$ to $x_{s}$ define whether the $i$-th element was the actual majority
element or a non-majority element.

Let,

\[
  x_{i} = \left\{
  \begin{array}{l l}
    0 & \quad \text{if the element was a non-majority element}\\
    1 & \quad \text{if the element was a majority element}\\
  \end{array}
  \right.
\]

Let, $X = \displaystyle\sum\limits_{i=1}^s{x_{i}}$,
given that atleast 55\% of the elements in the array are the majority elements,
$E[X] \geq \dfrac{55}{100}s$. We pick a false majority when $X < 50$. Now,
let us find the probability of that happening.

Using Chernoff Bounds, we know,

$Pr(X < (1-\delta)\mu) < \left(\dfrac{e^{-\delta}}{(1-\delta)^{(1-\delta)}}\right)^\mu$.

Now, $(1-\delta)\mu = \dfrac{50}{100}s$.

$(1-\delta)\dfrac{55}{100}s = \dfrac{50}{100}s$.

$\delta = \dfrac{1}{11}$.

Now, what is $Pr($We choose a wrong majority element$)?$

$ = Pr(X < \dfrac{50}{100}s) = \left(\dfrac{e^{-\frac{1}{11}}}{(\frac{10}{11})^{\frac{10}{11}}}\right)^{\frac{55}{100}s}$

$ = Pr(X < \dfrac{50}{100}s) = \left(\dfrac{e^{-\frac{1}{20}}}{(\frac{10}{11})^{\frac{1}{2}}}\right)^s$

$ = Pr(X < \dfrac{50}{100}s) = \left(\dfrac{1}{1.00234^{s}}\right)$

$ = Pr(X < \dfrac{50}{100}s) = \left(\dfrac{1}{1.00234^{c\log{n}}}\right)$

$ = Pr(X < \dfrac{50}{100}s) = \left(\dfrac{1}{n^{c\log{1.00234}}}\right)$

Hence, the probability of finding a fake majority is polynomially small for sufficiently large $c$.

\clearpage

\section{Balls \& Bins again}

We use the concept of \textit{indicator random variables} to solve
this problem\footnote{Partial solution from
  \url{http://math.stackexchange.com/questions/28930/another-balls-and-bins-question}}.

\subsection{Excepted number of empty bins}

After $n$ throws of balls, let $Y$ be the number of bins that have no
balls in them. Let $X_i$ be the indicator variable so that $X_i$ is
$1$ if bin $i$ is \textbf{empty} and is $0$ otherwise. Hence, 

$Y = X_1 + X_2 + X_3 + \ldots{} + X_n$

By the linearity of expectation,

$E[Y] = E[X_1] + E[X_2] + E[X_3] + \ldots{} + E[X_n]$

However, for every $i \in [1\ldots{}n]$, $E[X_i] = 0 \times P(X_i = 0)
+ 1 \times P(X_i = 1)$.\\
$P(X_i = 1) = $ the probability that bin $i$ is empty\\
$\Rightarrow P(X_i = 1) = $ the probability that all $n$ balls went
into a bin other than bin $i$\\
$\Rightarrow P(X_i = 1) = (1 - \frac{1}{n})^n$\\
$\therefore E[X_i] = 1 \times (1 - \frac{1}{n})^n$\\
$\Rightarrow E[Y] = n \times (1 - \frac{1}{n})^n$\\
$\Rightarrow E[Y] = n \times \frac{1}{e}$\\
$\Rightarrow E[Y] = \frac{n}{e}$


\subsection{Expected number of bins with exactly one ball}

After $n$ throws of balls, let $Y$ be the number of bins that hold
exactly one ball. Let $X_i$ be the indicator variable so that $X_i$ is
$1$ if bin $i$ has \textbf{exactly one} ball and is $0$
otherwise. Hence,

$Y = X_1 + X_2 + X_3 + \ldots{} + X_n$

By the linearity of expectation,

$E[Y] = E[X_1] + E[X_2] + E[X_3] + \ldots{} + E[X_n]$

However, for every $i \in [1\ldots{}n]$, $E[X_i] = 0 \times P(X_i = 0)
+ 1 \times P(X_i = 1)$.\\
$P(X_i = 1) = $ the probability that bin $i$ has exactly one ball\\
$\Rightarrow P(X_i = 1) = $ number of ways of selecting a ball \textit{and} the probability that one ball went into
bin $i$ \textit{and} all other other $(n-1)$ balls went into a bin other than
bin $i$\\
$\Rightarrow P(X_i = 1) = \binom{n}{1} \times \frac{1}{n} \times (1 - \frac{1}{n})^{(n-1)}$\\
$\therefore E[X_i] = n \times \frac{1}{n} \times \frac{1}{e}$\\
$\therefore E[X_i] = \frac{1}{e}$\\
$\Rightarrow E[Y] = n \times \frac{1}{e}$\\
$\Rightarrow E[Y] = \frac{n}{e}$

\clearpage

\section {Problem 2 Using W.H.P. Analysis}

\subsection{Negative Correlation}
Pr(Bin 1 is empty) = $\left(1-\dfrac{1}{n}\right)^n = \left(\dfrac{n-1}{n-2}\right)^n$\\
Pr(Bin 2 is empty | Bin 1 is empty) = $\left(1-\dfrac{1}{n-1}\right)^n = \left(\dfrac{n-2}{n-3}\right)^n$\\
Pr(Bin 3 is empty | Bins 1 and 2 are empty) = $\left(1-\dfrac{1}{n-2}\right)^n = \left(\dfrac{n-3}{n-2}\right)^n$\\
Pr(Bin k is empty | Bins 1, 2, .. k-1 are empty) = $\left(1-\dfrac{1}{n-(k-1)}\right)^n = \left(\dfrac{n-k}{n-k+1}\right)^n$\\


\subsection{$\theta(n)$ empty and $\theta(n)$ full bins, with high probability}
In Question 2, we proved that $E[$\# of empty bins$] = \dfrac{n}{e}$. \\
Therefore,
$E[$\# of non-empty bins$] = \left(n-\dfrac{n}{e}\right) = \left(\dfrac{ne-n}{e}\right)$.\\
\\
Now, let us prove that the probability of having $\leq \dfrac{n}{10e} $ empty bins
is polynomially small.\\
\\
Using Chernoff Bounds, we know,
\\
$Pr(X < (1-\delta)\mu) < \left(\dfrac{e^{-\delta}}{(1-\delta)^{(1-\delta)}}\right)^\mu$.
\\
Now, $(1-\delta)\mu = \dfrac{n}{10e}$\\
\\
$(1-\delta)\dfrac{n}{e} = \dfrac{n}{10e}$\\
\\
$\delta = \dfrac{9}{10}$\\
\\
$Pr\left(X < \dfrac{n}{10e}\right) < \left(\dfrac{e^{-\frac{9}{10}}}{(1-\frac{9}{10})^{(1-\frac{9}{10})}}\right)^{\frac{n}{e}}$\\
\\
$Pr\left(X < \dfrac{n}{10e}\right) < \left(\dfrac{e^{-\frac{9}{10}}}{(\frac{1}{10})^{(\frac{1}{10})}}\right)^{\frac{n}{e}}$\\
\\
$Pr\left(X < \dfrac{n}{10e}\right) < \left(\dfrac{1}{e^{\frac{9}{10}}(\frac{1}{10})^{(\frac{1}{10})}}\right)^{\frac{n}{e}}$\\
$Pr\left(X < \dfrac{n}{10e}\right) < \left(\dfrac{1}{1.2793^{n}}\right)$\\
\\
Thus, the probability of having $\leq \dfrac{n}{10e} $ empty bins
is exponentially small.\\
\\
Now, let us prove that the probability of having $\leq \dfrac{ne-n}{10e} $ full bins
is polynomially small.\\
\\
Using Chernoff Bounds, we know,
\\
$Pr(X < (1-\delta)\mu) < \left(\dfrac{e^{-\delta}}{(1-\delta)^{(1-\delta)}}\right)^\mu$.
\\
Now, $(1-\delta)\mu = \dfrac{ne-n}{10e}$\\
\\
$(1-\delta)\dfrac{ne-n}{e} = \dfrac{ne-n}{10e}$\\
\\
$\delta = \dfrac{9}{10}$\\
\\
$Pr\left(X < \dfrac{ne-n}{10e}\right) < \left(\dfrac{e^{-\frac{9}{10}}}{(1-\frac{9}{10})^{(1-\frac{9}{10})}}\right)^{\frac{ne-n}{e}}$\\
\\
$Pr\left(X < \dfrac{ne-n}{10e}\right) < \left(\dfrac{e^{-\frac{9}{10}}}{(\frac{1}{10})^{(\frac{1}{10})}}\right)^{\frac{ne-n}{e}}$\\
\\
$Pr\left(X < \dfrac{ne-n}{10e}\right) < \left(\dfrac{1}{e^{\frac{9}{10}}(\frac{1}{10})^{(\frac{1}{10})}}\right)^{\frac{ne-n}{e}}$\\
$Pr\left(X < \dfrac{ne-n}{10e}\right) < \left(\dfrac{1}{1.527^{n}}\right)$\\
\\
Therefore, the probability of having $\leq \dfrac{ne-n}{10e} $ full bins, is exponentially small.\\
\\
Hence, we can say that w.h.p, there are $\theta(n)$ empty and $\theta(n)$ full bins.

\subsection{Part (b) without Chernoff Bounds}
\clearpage

\section {Lower bound on the number of number of balls in the fullest bin}

\subsection{Number of balls in each bin}

Let $E[X] $ be the expected number of balls in a bin of size $n^{1/4}$.

$\therefore E[X] = n^{1/4}$

Let's find the probability that each block of $n^{1/4}$ bins contains $< \dfrac{n^{1/4}}{10}$ balls.

$\therefore (1-\delta)E[X] = \dfrac{n^{1/4}}{10}$

$\Rightarrow (1-\delta)n^{1/4} = \dfrac{n^{1/4}}{10}$

$\Rightarrow (1-\delta) = \frac{1}{10}$

$\Rightarrow \delta = \frac{9}{10}$

$\therefore P(X < (1-\delta)E[X]) < \left(\dfrac{e^{-9/10}}{0.1^{0.1}}\right)^{n^{1/4}}$

$< 0.511^{n^{1/4}}$

Which is exponentially small in $n$, for sufficiently large $n$.


\subsection{Lower bound on the probability that the first bin in each group contains $\Omega(\log{n}/\log{\log{n}})$ balls}

Let group 1 contain $n^{1/4}$ balls.

$\therefore Pr(1^{st}$ Bin in group-1 contains exactly $k$ Balls$) = \dbinom{n^{1/4}}{k} \left( \dfrac{1}{n^{1/4}} \right)^{k} \left( 1 - \dfrac{1}{n^{1/4}} \right)^{(n^{1/4} - k)}$

$\therefore Pr(1^{st}$ Bin in group-1 contains $\ge k$ Balls$) \ge \dbinom{n^{1/4}}{k} \left( \dfrac{1}{n^{1/4}} \right)^{k}$

$\ge \left(\dfrac{n^{1/4}}{k}\right)^{k} \left( \dfrac{1}{n^{1/4}} \right)^{k}$

$\ge \left(\dfrac{1}{k}\right)^{k}$

We substitute $k = \dfrac{c\log{n}}{\log{\log{n}}}$ in the equation above to get:

$\therefore Pr(1^{st}$ Bin in group-1 contains $\ge \frac{c\log{n}}{\log{\log{n}}}$ Balls$) \ge \left(\dfrac{1}{c\log{n}/\log{\log{n}}}\right)^{c\log{n}/\log{\log{n}}}$

$\ge \left(\dfrac{1}{{e}^{\log{c\log{n}/\log{\log{n}}}}}\right)^{c\log{n}/\log{\log{n}}}$

$\ge \left(\dfrac{1}{{e}^{(\log{c\log{n}} - \log{\log{\log{n}}})\frac{c\log{n}}{\log{\log{n}}}}} \right)$

$\ge \left(\dfrac{1}{{e}^{\frac{\log{c\log{n}} - \log{\log{\log{n}}}}{\log{\log{n}}}}} \right)$

$\ge \left(\dfrac{1}{{e}^{\frac{\log{c\log{n}} - \log{\log{\log{n}}}}{\log{\log{n}}}}} \right)$

\clearpage

\section {Simple multiple access channel}

\subsection{Backoff strategy to execute all packets in $\Theta(n)$ time when $n$ is \textbf{known}}

We know that if we throw $n$ balls in $n$ bins, $\frac{n}{e}$ of the
bins will have exactly one ball in them (in expectation). This means
that $\Theta(n)$ of the bins would have exactly one ball in them.

This means that a \textit{constant fraction} of the packets are
transmitted at every step. If we reduce the window size by a
\textit{constant fraction} at every step, we can be assured of the
transmission of all the packets. If this constant fraction is $< 1$,
then we can do this is time $\Theta(n)$ as well.

\subsection{Backoff strategy to execute all packets in $\Theta(n)$ time when $n$ is \textbf{not known}}

If we do not know the value of $n$, we can run the algorithm from the
previous step for $n = 1, 2, 4, 8, 16, 32, \ldots{}$. When we
determine that all the transmissions have been successful, we
stop. This will happen when $n = N$, where $N$ is the actual number of
transmitters.

The running time for this algorithm is $T(n) = T(n/2) + \Theta(n) =
\Theta(n)$.

\clearpage

\section{Estimating the number of players}
We have two approaches to solve this problem. Let the number of 
players be $n$.

\subsection{Using the W.H.P analysis in Question 3}
If we have an estimate for $n$, we would like to test if this is a good
estimate. Let's call testing if the estimate is good, a `phase'.

In each `phase', all the players transmit 1 packet, in any of the
$n$ windows available, randomly. Now, if our estimate of $n$, is correct,
we have $n$ packets to receive from $n$ players, and $n$ windows available.
Thus, this is analogous to the problem of throwing $n$ balls in $n$ bins.

Now, from problem 3, we know that if we have $n$ bins and $n$ balls, the number of empty
bins would be $\dfrac{n}{e}$ with high probability, for large n. If the
number of empty bins is significantly less than $\dfrac{n}{e}$, that means
the number of bins is far too fewer than the number of balls. Hence, we 
double our estimate of $n$.

We start with $n = 2$, and keep doubling our estimate until we reach a good
estimate. When we reach the closest power of 2, greater than $n$, the 
number of empty bins, would be approximately $\dfrac{n}{e}$ w.h.p., as we proved earlier
in Question 3, and our estimate would be at-most a constant factor (2) bigger
than the actual value of $n$, with high probability

\subsection{Decreasing Number of Windows by a factor of 2}
Here, we start with an estimate of $n = 2$, and in each phase we have at most $\log_2{n}$
rounds. Starting from the first round, where we have $n$ windows to transmit, 
the number of windows in each round progressively decreases by a factor of 2.

In a phase, each player tries to transmit once. If it succeeds, it does not
transmit anymore in the phase. We progressively decrease the number of windows 
by a factor of 2 each time, until the number of windows becomes 1, or there are 
no more transmissions.

If our estimate is good, then the claim is, we would terminate before we run out
of the number of rounds, because as proved in problem 3, we reduce the number of
players who haven't transmitted yet by a constant fraction.

If our estimate is not good, we double our estimate, and try again. Thus, our 
estimate would be at most a constant times bigger than the actual value of $n$,
with high probability.

\end{document}
