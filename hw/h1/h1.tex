\documentclass{article}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{url}
\usepackage{color}
\usepackage{savetrees}
\usepackage{listings}
\usetikzlibrary{shapes}


\linespread{1.5}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1.9ex plus 0.5ex minus 0.2ex}

\title{CSE638 \\ Advanced Graduate Algorithms (and Data Structures) -- Homework-1}

\author{Dhruv Matani (108267786) \& Gaurav Menghani (108266803)}

\begin{document}
\maketitle

\clearpage

\tableofcontents

\clearpage

\section{Proof of Linearity of Expectation}

Show that: $E(cX + Y) = cE(X) + E(Y)$, where\\
$c$ is a constant, and $X$ \& $Y$ are random variables.\\
Let $S$ \& $T$ be the set of values that $X$ \& $Y$ respectively can take on.

Lemma-1: $P(X=k) = \displaystyle\sum\limits_{m \in T} P(X=k\ and\ Y=m)$\\
$E(cX = Y) = \displaystyle\sum\limits_{k \in S} \displaystyle\sum\limits_{m \in T} (ck + m) P(X=k\ and\ Y=m)$\\
$= \displaystyle\sum\limits_{k \in S} \displaystyle\sum\limits_{m \in T} ck P(X=k\ and\ Y=m) + \displaystyle\sum\limits_{m \in T} \displaystyle\sum\limits_{k \in S} m P(X=k\ and\ Y=m)$\\
$= c\displaystyle\sum\limits_{k \in S} k \displaystyle\sum\limits_{m \in T} P(X=k\ and\ Y=m) + \displaystyle\sum\limits_{m \in T} m \displaystyle\sum\limits_{k \in S} P(X=k\ and\ Y=m)$\\
$= c\displaystyle\sum\limits_{k \in S} k P(X=k) + \displaystyle\sum\limits_{m \in T} P(Y=m)$ \ldots{} (using Lemma-1)\\
$= cE(X) + E(Y)$\footnote{Proof copied from: \url{https://www.youtube.com/watch?v=PicS-yWlZRw}}\\

\clearpage

\section{Coin Flip Problem}
If claim that if we flip the coin $c \log{n}$ times, where both $c$ and
$n$ are large, we will get a head.

Let us find the probability of not getting a head after $c \log{n}$ flips.\\
${= (1-p)^{c\log{n}}}$ \\
Let $1-p = \dfrac{1}{q}$ \\
$= \dfrac{1}{q^{c\log{n}}}$ \\
$= \dfrac{1}{n^{c\log{q}}} $\\
Thus, for large enough $n$ and $c$, the probability is polynomially small
that we would not have a head even after $c \log{n}$ coin flips.
\clearpage


\section{Balls \& Hats (Secretary Hiring Problem)}
This is a randomization problem. As hinted by Prof. Bender in office hours of CSE548,
this is a problem analogous to finding a suitable life-partner. It might not
be optimal to marry the first person you date, nor would it be optimal to 
wait till you are 60. The line has to be drawn somewhere in between.\\
\\
The approach \footnote{Partial solution from: \url{http://en.wikipedia.org/wiki/Secretary_problem}} 
is to reject a constant fraction of the balls that you encounter
first, and then choose the first ball which has the greatest number seen so far. 
Let us assume we decide to reject the first $\frac{1}{c}$ fraction of
the balls we encounter. We only need to figure out $c$ to be sufficient enough
to be correct $\frac{1}{4}$ of the time, or better.\\
\\
Let $\frac{n}{c} = r$. This means, we reject the first $r$ balls. Now, we have
the next $n-r$ balls to choose from. What is the probability that using the 
above-mentioned strategy, we choose a ball from these $n-r$ balls and it turns 
out to have the biggest number?\\
\\
Assume we chose ball $i$, where $i > r$, \\
$P($ball $i$ was chosen and is the best$) = P($ ball $i$ is the best$) \times P($ 
the best ball amongst the first $i-1$ balls lies in the first $r$ balls$)$.\\
\\
$P($ ball $i$ is the best$) = \frac{1}{n}$. The second probability deals with the 
fact that if the best ball amongst the first $i-1$ balls did not lie in the first 
$r$ balls, the $i$-th ball wouldn't have had been chosen.This probability is, 
$\frac{\binom{r}{1}}{\binom{i-1}{1}}$, which is, $\frac{r}{i-1}$.\\
\\
Thus, $P($ball $i$ was chosen and is the best$) = \frac{1}{n} \times \frac{r}{i-1}$.\\
\\
Since $i$ can vary between $r+1$ to $n$, the total probability of winning becomes,\\
$P(winning) = \displaystyle\sum\limits_{i=r+1}^n P($ ball $i$ was chosen and is the best$)$.\\
\\
$P(winning) = \displaystyle\sum\limits_{i=r+1}^n \frac{1}{n} \times \frac{r}{i-1}$.\\
$P(winning) = \frac{r}{n} \displaystyle\sum\limits_{i=r+1}^n \frac{1}{i-1}$\\
By using the formula for harmonic series \footnote{\url{http://en.wikipedia.org/wiki/Harmonic_series_(mathematics)}}, 
this can be simplified to,\\
$P(winning) = \frac{r}{n} (\log{n} - \log{r})$\\
$P(winning) = -\frac{r}{n} (\log{r} - \log{n})$\\
$P(winning) = -\frac{r}{n}(\log{\frac{r}{n}})$\\
\\
Since, $\frac{n}{c} = r$, therefore,\\
$P(winning) = - c \log{c}$\\
Since, $P(winning) \geq \frac{1}{4}$,\\
$P(winning) = - c \log{c} \geq \frac{1}{4}$,\\
Solving this, we get, $c = 0.6994905768$. Thus, if we reject nearly first 69.94\% of the balls,
we would win $\frac{1}{4}$ of the time or more.
\clearpage

\section{Probabilistic Ball Throwing Problem}
\subsection{All balls are blue}
The probability of a blue ball is $\frac{1}{2}$, hence the probability of
$n$ blue balls is $\dfrac{1}{2^n}$.

\subsection{$\frac{n}{2}$ balls are blue, and $\frac{n}{2}$ are red}
Pr($\frac{n}{2}$ balls are blue and $\frac{n}{2}$ balls are red) = (Number of ways of choosing $\frac{n}{2}$
balls which will be blue) $\times$ Pr($\frac{n}{2}$ are blue) $\times$
Pr($\frac{n}{2}$ are red).

$= \dbinom{n}{\frac{n}{2}} \times \dfrac{1}{2^{\frac{n}{2}}} \times \dfrac{1}{2^{\frac{n}{2}}}$

$= \dbinom{n}{\frac{n}{2}} \times \dfrac{1}{2^{n}}$

\subsection{All balls thrown in the bin are blue $B_{n} = n-1$}
Initially there is one blue ball and one red ball. Rest of the $n-2$ balls will
all need to be blue. As per the formula, the probability of the next ball 
being blue is, $\frac{1}{2}$. Then, we would have 2 blue balls and one red ball.
Therefore, the probability of the ball after that being blue is $\frac{2}{3}$,
and so on.

Therefore, the cumulative probaility is, \\
$= \dfrac{1}{2} \times \dfrac{2}{3} \times \dfrac{3}{4} \times ... \times \dfrac{n-3}{n-2} \times \dfrac{n-2}{n-1}$ \\
$= \dfrac{1}{n-1}$ \\
Thus, the probability of having $B_{n} = n-1$ is $\dfrac{1}{n-1}$.
\subsection{$B_{n} = \dfrac{n}{2}$}
Whenever a ball is drawn, the probability of it being a 
particular color is $\dfrac{B_{t}}{B_{t} + R_{t}}$, or
$\dfrac{R_{t}}{B_{t}+R_{t}}$, the cumulative probability
is the product of these probabilities.

Since we want, both $B_{n} = \dfrac{n}{2}$ and 
$R_{n} = \dfrac{n}{2}$, in the end, and the value 
$B_{t}+R_{t}$ in the denominator will keep increasing
from 2 onwards to $n-1$, since we are continuosly adding
balls. Thus the value of the denominator will be $(n-1)!$.

It is also easy to see that when we add blue balls, we
would be multiplying the numerator in an increasing 
sequence from 2 onwards to $\dfrac{n}{2}-1$, since we
need to have $B_{n} = \dfrac{n}{2}$. Thus, we would
be multiplying the numerator by $\left(\dfrac{n}{2}-1\right)!$.
We would do the same for red balls, and hence in all
we would be multiplying the numerator by 
$\left(\left(\dfrac{n}{2}-1\right)!\right)^2$. \\

Therefore,

Pr(A sequence of draws occurs which ends with $B_{n} = \frac{n}{2}$) $ = \dfrac{((\frac{n}{2}-1)!)^2}{(n-1)!} $.

We can find the number of such sequence of draws, by finding the
number of ways in which we can order the drawing of red or blue
balls. This is $\dbinom{n-2}{\frac{n}{2}-1}$.

Hence,

Pr($B_n = \frac{n}{2}$) $ = \dbinom{n-2}{\frac{n}{2}-1} \times
\dfrac{((\frac{n}{2}-1)!)^2}{(n-1)!}$

$= \dfrac{(n-2)!}{((\frac{n}{2}-1)!)^2} \times \dfrac{((\frac{n}{2}-1)!)^2}{(n-1)!}$

$= \dfrac{1}{n-1}$

\clearpage

\section{Dinner Party Problem}
For this problem we took help from 
\footnote{http://math.stackexchange.com/questions/5595/taking-seats-on-a-plane}.

Here are a few observations: 

When it is the last person's turn, only one of two seats can be left. 
Either the first, or the last. We can prove this as follows. There is definitely 
at least one seat left since there are $n$ seats and $n-1$ would have had made 
choices till then. Also, it is not possible for a seat $i$ to be
left at the end, where $i \notin  \{ 1, n \}$. Because if it was, the person $i$ would
have chosen that seat when it was his/her choice. 

When any person makes a choice between either seat $1$ or seat $n$, it is a decisive choice.
If seat $n$ is taken, person $n$ won't be able take his/her choice. If the seat
$1$ is taken, then all the following people would get the seats of their choice, and
hence person $n$ would get the seat assigned to him/her. If any other seat is 
chosen, it won't have any effect on the outcome, and this decisive choice
is simply postponed. 

Thus, the choice is always made once, and the outcome only depends upon this one
choice. Both the options could be chosen with a probability $\frac{1}{2}$ each. It is
only if when the seat $1$ is chosen, that person $n$ would get to sit in his/her place
and this choice is made with a probability of $\frac{1}{2}$. Therefore, the person
$n$ gets to sit in his/her seat with probability $\frac{1}{2}$. 


\clearpage

\section{Hash Table Problem}
\subsection{In $c\log{n}$ contiguous bins, there are strictly
fewer than $c\log{n}$ balls.}

To show that a run of $c\log{n}$ bins does not contain more than
$c\log{n}$ balls, we show that each bin in a mutually exclusive set of run of bins
of size $\dfrac{c}{2}\log{n}$ does not contain more than
$\dfrac{c}{3}\log{n}$ balls with high probability.

Let $E[X] =$ The expected number of balls in a run of $d\log{n}$ bins.

$\Rightarrow E[X] = \frac{1}{2}d\log{n}$

We know that $Pr(X > (1+\delta)E[X]) \le \left( \dfrac{e^\delta}{(1+\delta)^{(1+\delta)}} \right)^{E[x]}$ if $\delta > 0$

If $(1+\delta)E[X] = \dfrac{2d}{3}\log{n}$, then

$(1+\delta)\dfrac{1}{2}d\log{n} = \dfrac{2d}{3}\log{n}$

$\Rightarrow (1+\delta)\frac{1}{2} = \frac{2}{3}$

$\Rightarrow (1+\delta) = \frac{4}{3}$

$\Rightarrow \delta = \dfrac{1}{3}$

$\therefore Pr(X > \dfrac{2d}{3}\log{n}) \le \left( \dfrac{e^{1/3}}{{4/3}^{4/3}} \right)^{\frac{1}{2}d\log{n}}$

$\le \left( \dfrac{1.4}{1.47} \right)^{\frac{1}{2}d\log{n}}$

$\le 0.95^{\frac{1}{2}d\log{n}}$

$\le n^{\frac{1}{2}d\log{0.95}}$

$\le \dfrac{1}{n^{0.5 \times \log{1.0515} \times d}}$

$\le \dfrac{1}{n^{0.5 \times 0.07247 \times d}}$

$\le \dfrac{1}{n^{0.0362d}}$

Let $d = \dfrac{c}{2}\log{n}$

$\therefore Pr(X > \frac{c}{2}\log{n}) \le \dfrac{1}{n^{0.0181c}}$

For sufficiently large $c$, $\dfrac{1}{n^{0.0181c}}$ is polynomially small.

Hence, if we take any block of size $\frac{c}{2}\log{n}$ bins (out of
the total blocks being ${2n}/{\frac{c}{2}\log{n}}$), then we will
not have more than $\frac{c}{3}\log{n}$ balls in any consecutive
block. It follows that a run of $c\log{n}$ bins will NOT have $>
c\log{n}$ balls with high probability.

Thus, the probability is polynomially small. Hence, with high
probability, there is no selection of $c\log{n}$ bins with more than
$c\log{n}$ balls.

\subsection{Why this proves that the longest sequence of probes
is $O(\log{n})?$}

Since with high probability, there is no selection of $c\log{n}$ bins
with more than $c\log{n}$ balls, we can analogously say that if we
were to hash $n$ elements into $2n$ slots, there would be no selection
of $c\log{n}$ contiguous slots which have more than $c\log{n}$ balls
with high probability.

Since there would be no such selection of slots, the length of the
longest probe would be $O(\log{n})$.

\clearpage



\end{document}
