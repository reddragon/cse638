\documentclass{article}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{tikz}
\usepackage{url}
\usepackage{color}
\usepackage{savetrees}
\usepackage{listings}
\usetikzlibrary{shapes}


\linespread{1.5}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1.9ex plus 0.5ex minus 0.2ex}

\title{CSE638 \\ Advanced Graduate Algorithms (and Data Structures) -- Homework-1}

\author{Dhruv Matani (108267786) \& Gaurav Menghani (108266803)}

\begin{document}
\maketitle

\clearpage

\tableofcontents

\clearpage

\section{Proof of Linearity of Expectation}

Show that: $E(cX + Y) = cE(X) + E(Y)$, where\\
$c$ is a constant, and $X$ \& $Y$ are random variables.\\
Let $S$ \& $T$ be the set of values that $X$ \& $Y$ respectively can take on.

Lemma-1: $P(X=k) = \displaystyle\sum\limits_{m \in T} P(X=k\ and\ Y=m)$\\
$E(cX = Y) = \displaystyle\sum\limits_{k \in S} \displaystyle\sum\limits_{m \in T} (ck + m) P(X=k\ and\ Y=m)$\\
$= \displaystyle\sum\limits_{k \in S} \displaystyle\sum\limits_{m \in T} ck P(X=k\ and\ Y=m) + \displaystyle\sum\limits_{m \in T} \displaystyle\sum\limits_{k \in S} m P(X=k\ and\ Y=m)$\\
$= c\displaystyle\sum\limits_{k \in S} k \displaystyle\sum\limits_{m \in T} P(X=k\ and\ Y=m) + \displaystyle\sum\limits_{m \in T} m \displaystyle\sum\limits_{k \in S} P(X=k\ and\ Y=m)$\\
$= c\displaystyle\sum\limits_{k \in S} k P(X=k) + \displaystyle\sum\limits_{m \in T} P(Y=m)$ \ldots{} (using Lemma-1)\\
$= cE(X) + E(Y)$\footnote{Proof copied from: \url{https://www.youtube.com/watch?v=PicS-yWlZRw}}\\

\clearpage

\section{Coin Flip Problem}
If claim that if we flip the coin $c \log{n}$ times, where both $c$ and
$n$ are large, we will get a head.

Let us find the probability of not getting a head after $c \log{n}$ flips.\\
${= (1-p)^{c\log{n}}}$ \\
Let $1-p = \dfrac{1}{q}$ \\
$= \dfrac{1}{q^{c\log{n}}}$ \\
$= \dfrac{1}{n^{c\log{q}}} $\\
Thus, for large enough $n$ and $c$, the probability is polynomially small
that we would not have a head even after $c \log{n}$ coin flips.
\clearpage


\section{Balls \& Hats (Secretary Hiring Problem)}
This is a randomization problem. As hinted by Prof. Bender in office hours of CSE548,
this is a problem analogous to finding a suitable life-partner. It might not
be optimal to marry the first person you date, nor would it be optimal to 
wait till you are 60. The line has to be drawn somewhere in between.\\
\\
The approach \footnote{Partial solution from: \url{http://en.wikipedia.org/wiki/Secretary_problem}} 
is to reject a constant fraction of the balls that you encounter
first, and then choose the first ball which has the greatest number seen so far. 
Let us assume we decide to reject the first $\frac{1}{c}$ fraction of
the balls we encounter. We only need to figure out $c$ to be sufficient enough
to be correct $\frac{1}{4}$ of the time, or better.\\
\\
Let $\frac{n}{c} = r$. This means, we reject the first $r$ balls. Now, we have
the next $n-r$ balls to choose from. What is the probability that using the 
above-mentioned strategy, we choose a ball from these $n-r$ balls and it turns 
out to have the biggest number?\\
\\
Assume we chose ball $i$, where $i > r$, \\
$P($ball $i$ was chosen and is the best$) = P($ ball $i$ is the best$) \times P($ 
the best ball amongst the first $i-1$ balls lies in the first $r$ balls$)$.\\
\\
$P($ ball $i$ is the best$) = \frac{1}{n}$. The second probability deals with the 
fact that if the best ball amongst the first $i-1$ balls did not lie in the first 
$r$ balls, the $i$-th ball wouldn't have had been chosen.This probability is, 
$\frac{\binom{r}{1}}{\binom{i-1}{1}}$, which is, $\frac{r}{i-1}$.\\
\\
Thus, $P($ball $i$ was chosen and is the best$) = \frac{1}{n} \times \frac{r}{i-1}$.\\
\\
Since $i$ can vary between $r+1$ to $n$, the total probability of winning becomes,\\
$P(winning) = \displaystyle\sum\limits_{i=r+1}^n P($ ball $i$ was chosen and is the best$)$.\\
\\
$P(winning) = \displaystyle\sum\limits_{i=r+1}^n \frac{1}{n} \times \frac{r}{i-1}$.\\
$P(winning) = \frac{r}{n} \displaystyle\sum\limits_{i=r+1}^n \frac{1}{i-1}$\\
By using the formula for harmonic series \footnote{\url{http://en.wikipedia.org/wiki/Harmonic_series_(mathematics)}}, 
this can be simplified to,\\
$P(winning) = \frac{r}{n} (\log{n} - \log{r})$\\
$P(winning) = -\frac{r}{n} (\log{r} - \log{n})$\\
$P(winning) = -\frac{r}{n}(\log{\frac{r}{n}})$\\
\\
Since, $\frac{n}{c} = r$, therefore,\\
$P(winning) = - c \log{c}$\\
Since, $P(winning) \geq \frac{1}{4}$,\\
$P(winning) = - c \log{c} \geq \frac{1}{4}$,\\
Solving this, we get, $c = 0.6994905768$. Thus, if we reject nearly first 69.94\% of the balls,
we would win $\frac{1}{4}$ of the time or more.
\clearpage

\section{Probabilistic Ball Throwing Problem}
\subsection{All balls are blue}
The probability of a blue ball is $\frac{1}{2}$, hence the probability of
$n$ blue balls is $\dfrac{1}{2^n}$.

\subsection{$\frac{n}{2}$ balls are blue, and $\frac{n}{2}$ are red}
Pr($\frac{n}{2}$ balls are blue and $\frac{n}{2}$ balls are red) = (Number of ways of choosing $\frac{n}{2}$
balls which will be blue) $\times$ Pr($\frac{n}{2}$ are blue) $\times$
Pr($\frac{n}{2}$ are red).

$= \dbinom{n}{\frac{n}{2}} \times \dfrac{1}{2^{\frac{n}{2}}} \times \dfrac{1}{2^{\frac{n}{2}}}  $ \\

\subsection{All balls thrown in the bin are blue $B_{n} = n-1$}
Initially there is one blue ball and one red ball. Rest of the $n-2$ balls will
all need to be blue. As per the formula, the probability of the next ball 
being blue is, $\frac{1}{2}$. Then, we would have 2 blue balls and one red ball.
Therefore, the probability of the ball after that being blue is $\frac{2}{3}$,
and so on.

Therefore, the cumulative probaility is, \\
$= \dfrac{1}{2} \times \dfrac{2}{3} \times \dfrac{3}{4} \times ... \times \dfrac{n-3}{n-2} \times \dfrac{n-2}{n-1}$ \\
$= \dfrac{1}{n-1}$ \\
Thus, the probability of having $B_{n} = n-1$ is $\dfrac{1}{n-1}$.
\subsection{$B_{n} = \dfrac{n}{2}$}
Whenever a ball is drawn, the probability of it being a 
particular color is $\dfrac{B_{t}}{B_{t} + R_{t}}$, or
$\dfrac{R_{t}}{B_{t}+R_{t}}$, the cumulative probability
is the product of these probabilities.

Since we want, both $B_{n} = \dfrac{n}{2}$ and 
$R_{n} = \dfrac{n}{2}$, in the end, and the value 
$B_{t}+R_{t}$ in the denominator will keep increasing
from 2 onwards to $n-1$, since we are continuosly adding
balls. Thus the value of the denominator will be $(n-1)!$.

It is also easy to see that when we add blue balls, we
would be multiplying the numerator in an increasing 
sequence from 2 onwards to $\dfrac{n}{2}-1$, since we
need to have $B_{n} = \dfrac{n}{2}$. Thus, we would
be multiplying the numerator by $(\frac{n}{2}-1)!$.
We would do the same for red balls, and hence in all
we would be multiplying the numerator by 
$(\frac{n}{2}-1)^2$. \\

Therefore,\\
Pr(A sequence of draws occurs which ends with $B_{n} = \frac{n}{2}$) $ = \dfrac{(\frac{n}{2}-1)^2}{(n-1)!} $.
\\
\\
We can find the number of such sequence of draws, by
finding the number of ways in which we can order the
drawing of red or blue balls. This is $\dbinom{n-2}{\frac{n}{2}-1}$.

Hence, \\
Pr($B_n = \frac{n}{2}$) $ = 
\dbinom{n-2}{\frac{n}{2}-1} \times
\dfrac{(\frac{n}{2}-1)^2}{(n-1)!}$.  
\clearpage

\section{Dinner Party Problem}
TODO Clarify

\clearpage

\section{Hash Table Problem}
\subsection{In $c\log{n}$ contiguous bins, there are strictly
fewer than $c\log{n}$ balls.}
\begin{enumerate}

\item Q: In how many ways can we select $c\log{n}$ contiguous bins?

  A: $\dbinom{2n-c\log{n}}{1}  = (2n-c\log{n})$.

\item Q: If we select one contiguous run of $c\log{n}$ bins, and throw
  $n$ balls, what is the probability that $c\log{n}$ balls fall into
  our selection of $c\log{n}$ bins?

  A: $\left(\dfrac{c\log{n}}{2n}\right)^{c\log{n}} \times 
  \left( \dfrac{2n-c\log{n}}{2n} \right) ^{n-c\log{n}}$.

\end{enumerate}

Hence, the probability of the existence of a contiguous
sequence of $c\log{n}$ bins, with $c\log{n}$ balls is: \\
$ = (2n-c\log{n}) \times \left(\dfrac{c\log{n}}{2n}\right)^{c\log{n}} \times 
\left( \dfrac{2n-c\log{n}}{2n} \right) ^{n-c\log{n}}$ \\
$ \leq (2n-c\log{n}) \times \left(\dfrac{c\log{n}}{2n}\right)^{c\log{n}}$ \\
$ \leq (2n) \times \left( \dfrac{c\log{n}}{2n}\right)^{c\log{n}} $ \\
\\ When $n$ is sufficiently large, \\
$ \leq (2n) \times \left( \dfrac{1}{n} \right)^{c\log{n}}$ \\
$ \leq \dfrac{2}{n^{c\log{n}-1}} $ \\
\\ Thus, the probability is polynomially small. Hence, with high
probability, there is no selection of $c\log{n}$ bins with
more than $c\log{n}$ balls.

TODO Check

\subsection{Why this proves that the longest sequence of probes
is $O(\log{n})?$}


Since with high probability, there is no selection of $c\log{n}$ bins
with more than $c\log{n}$ balls, we can analogously say that if we
were to hash $n$ elements into $2n$ slots, there would be no selection of contiguous $c\log{n}$ slots which have
more than $c\log{n}$ balls with high probability.

Since there would be no such selection of slots, the length of the
longest probe would be $O(\log{n})$.

\clearpage



\end{document}
